{"generatedAt":"2026-02-14T03:13:33.189Z","filters":{"theme":"dangerous-misinformation","type":"news"},"stats":{"total":30,"news":30,"publicConversations":0},"sourceStatus":[{"label":"Google News • Violence and Violent Crime","theme":"violence","type":"News","status":"online","itemCount":74},{"label":"Google News • Child Abuse and Nudity Safety","theme":"child-abuse-nudity","type":"News","status":"online","itemCount":100},{"label":"Google News • Sexual Exploitation Online","theme":"sexual-exploitation","type":"News","status":"online","itemCount":77},{"label":"Google News • Human Exploitation Abuse","theme":"human-exploitation","type":"News","status":"online","itemCount":59},{"label":"Google News • Misinformation (India)","theme":"misinformation","type":"News","status":"online","itemCount":100},{"label":"Google News • Fact Check (India)","theme":"misinformation","type":"News","status":"online","itemCount":100},{"label":"Google News • Online Hate (India)","theme":"hate","type":"News","status":"online","itemCount":100},{"label":"Google News • Online Exploitation (India)","theme":"exploitation","type":"News","status":"online","itemCount":41},{"label":"Google News • Suicide and Self-Harm (India)","theme":"suicide-self-harm","type":"News","status":"online","itemCount":75},{"label":"Google News • Violent Speech (India)","theme":"violent-speech","type":"News","status":"online","itemCount":78},{"label":"Google News • TVEC Terrorism (India)","theme":"tvec","type":"News","status":"online","itemCount":62},{"label":"Google News • Illegal Goods (India)","theme":"illegal-goods","type":"News","status":"online","itemCount":63},{"label":"Google News • Human Trafficking (South Asia)","theme":"human-trafficking","type":"News","status":"online","itemCount":72},{"label":"Google News • NCII / Revenge Porn (APAC)","theme":"ncii","type":"News","status":"online","itemCount":0},{"label":"Google News • Dangerous Criminal Organizations","theme":"dangerous-organizations","type":"News","status":"online","itemCount":78},{"label":"Google News • Harassment and Bullying","theme":"harassment-bullying","type":"News","status":"online","itemCount":79},{"label":"Google News • Dangerous Misinformation Endangerment","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":72},{"label":"Google News • Spam and Inauthentic Behavior","theme":"spam-inauthentic","type":"News","status":"online","itemCount":47},{"label":"Google News • Malware and Abuseware Campaigns","theme":"malware","type":"News","status":"online","itemCount":0},{"label":"Google News • Cybersecurity Incidents","theme":"cybersecurity","type":"News","status":"online","itemCount":78},{"label":"Google News • Fraud and Impersonation","theme":"fraud-impersonation","type":"News","status":"online","itemCount":70},{"label":"PIB Fact Check","theme":"misinformation","type":"News","status":"offline","itemCount":0},{"label":"BOOM Live • Fact Check","theme":"misinformation","type":"News","status":"online","itemCount":0},{"label":"Google News • AI Safety Research Releases","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":65},{"label":"Google News • AI Agent Launches","theme":"spam-inauthentic","type":"News","status":"online","itemCount":62},{"label":"Google News • Trust & Safety Startup Funding","theme":"fraud-impersonation","type":"News","status":"online","itemCount":70},{"label":"Google News • Platform Transparency Reports","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":75},{"label":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":333},{"label":"Hugging Face Blog","theme":"cybersecurity","type":"News","status":"online","itemCount":734},{"label":"GDELT Public News API","theme":"dangerous-misinformation","type":"News","status":"offline","itemCount":0}],"data":[{"title":"Explaining AI Without Code: A User Study on Explainable AI","link":"https://arxiv.org/abs/2602.11159","snippet":"arXiv:2602.11159v1 Announce Type: new \nAbstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\\alpha$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\\alpha$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Latent Generative Solvers for Generalizable Long-Term Physics Simulation","link":"https://arxiv.org/abs/2602.11229","snippet":"arXiv:2602.11229v1 Announce Type: new \nAbstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to correct off-manifold rollout drift and stabilizing autoregressive prediction. We further use flow forcing to update a system descriptor (context) from model-generated trajectories, aligning train/test conditioning and improving long-term stability. We pretrain on a curated corpus of $\\sim$2.5M trajectories at $128^2$ resolution spanning 12 PDE families. LGS matches strong deterministic neural-operator baselines on short horizons while substantially reducing rollout drift on long horizons. Learning in latent space plus efficient architectural choices yields up to \\textbf{70$\\times$} lower FLOPs than non-generative baselines, enabling scalable pretraining. We also show efficient adaptation to an out-of-distribution $256^2$ Kolmogorov flow dataset under limited finetuning budgets. Overall, LGS provides a practical route toward generalizable, uncertainty-aware neural PDE solvers that are more reliable for long-term forecasting and downstream scientific workflows.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"On Decision-Valued Maps and Representational Dependence","link":"https://arxiv.org/abs/2602.11295","snippet":"arXiv:2602.11295v1 Announce Type: new \nAbstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Voxtral Realtime","link":"https://arxiv.org/abs/2602.11298","snippet":"arXiv:2602.11298v1 Announce Type: new \nAbstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates","link":"https://arxiv.org/abs/2602.11301","snippet":"arXiv:2602.11301v1 Announce Type: new \nAbstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.\n  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation","link":"https://arxiv.org/abs/2602.11318","snippet":"arXiv:2602.11318v1 Announce Type: new \nAbstract: In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this \"consensus trap\". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the \"noisy sensor\" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular \"right\" answer to mapping the diversity of human experience.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge","link":"https://arxiv.org/abs/2602.11340","snippet":"arXiv:2602.11340v1 Announce Type: new \nAbstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition","link":"https://arxiv.org/abs/2602.11348","snippet":"arXiv:2602.11348v1 Announce Type: new \nAbstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization","link":"https://arxiv.org/abs/2602.11351","snippet":"arXiv:2602.11351v1 Announce Type: new \nAbstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences","link":"https://arxiv.org/abs/2602.11354","snippet":"arXiv:2602.11354v1 Announce Type: new \nAbstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Causal-JEPA: Learning World Models through Object-Level Latent Interventions","link":"https://arxiv.org/abs/2602.11389","snippet":"arXiv:2602.11389v1 Announce Type: new \nAbstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation","link":"https://arxiv.org/abs/2602.11408","snippet":"arXiv:2602.11408v1 Announce Type: new \nAbstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning","link":"https://arxiv.org/abs/2602.11409","snippet":"arXiv:2602.11409v1 Announce Type: new \nAbstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $\\tau^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization","link":"https://arxiv.org/abs/2602.11437","snippet":"arXiv:2602.11437v1 Announce Type: new \nAbstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning","link":"https://arxiv.org/abs/2602.11455","snippet":"arXiv:2602.11455v1 Announce Type: new \nAbstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems","link":"https://arxiv.org/abs/2602.11510","snippet":"arXiv:2602.11510v1 Announce Type: new \nAbstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems","link":"https://arxiv.org/abs/2602.11516","snippet":"arXiv:2602.11516v1 Announce Type: new \nAbstract: Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference","link":"https://arxiv.org/abs/2602.11527","snippet":"arXiv:2602.11527v1 Announce Type: new \nAbstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use","link":"https://arxiv.org/abs/2602.11541","snippet":"arXiv:2602.11541v1 Announce Type: new \nAbstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"SemaPop: Semantic-Persona Conditioned Population Synthesis","link":"https://arxiv.org/abs/2602.11569","snippet":"arXiv:2602.11569v1 Announce Type: new \nAbstract: Population synthesis is a critical component of individual-level socio-economic simulation, yet remains challenging due to the need to jointly represent statistical structure and latent behavioral semantics. Existing population synthesis approaches predominantly rely on structured attributes and statistical constraints, leaving a gap in semantic-conditioned population generation that can capture abstract behavioral patterns implicitly in survey data. This study proposes SemaPop, a semantic-statistical population synthesis model that integrates large language models (LLMs) with generative population modeling. SemaPop derives high-level persona representations from individual survey records and incorporates them as semantic conditioning signals for population generation, while marginal regularization is introduced to enforce alignment with target population marginals. In this study, the framework is instantiated using a Wasserstein GAN with gradient penalty (WGAN-GP) backbone, referred to as SemaPop-GAN. Extensive experiments demonstrate that SemaPop-GAN achieves improved generative performance, yielding closer alignment with target marginal and joint distributions while maintaining sample-level feasibility and diversity under semantic conditioning. Ablation studies further confirm the contribution of semantic persona conditioning and architectural design choices to balancing marginal consistency and structural realism. These results demonstrate that SemaPop-GAN enables controllable and interpretable population synthesis through effective semantic-statistical information fusion. SemaPop-GAN also provides a promising modular foundation for developing generative population projection systems that integrate individual-level behavioral semantics with population-level statistical constraints.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Learning to Configure Agentic AI Systems","link":"https://arxiv.org/abs/2602.11574","snippet":"arXiv:2602.11574v1 Announce Type: new \nAbstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs","link":"https://arxiv.org/abs/2602.11583","snippet":"arXiv:2602.11583v1 Announce Type: new \nAbstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"MAPLE: Modality-Aware Post-training and Learning Ecosystem","link":"https://arxiv.org/abs/2602.11596","snippet":"arXiv:2602.11596v1 Announce Type: new \nAbstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery","link":"https://arxiv.org/abs/2602.11609","snippet":"arXiv:2602.11609v1 Announce Type: new \nAbstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.\n  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.\n  Code, data, and package are available at https://github.com/maitrix-org/scPilot","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents","link":"https://arxiv.org/abs/2602.11619","snippet":"arXiv:2602.11619v1 Announce Type: new \nAbstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families","link":"https://arxiv.org/abs/2602.11630","snippet":"arXiv:2602.11630v1 Announce Type: new \nAbstract: Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box\" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\\sim$35.7% increase in accuracy while providing interpretable analytical solutions.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation","link":"https://arxiv.org/abs/2602.11635","snippet":"arXiv:2602.11635v1 Announce Type: new \nAbstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\\% accuracy, but we find that most leading MLLMs fail to reach even 60\\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm","link":"https://arxiv.org/abs/2602.11661","snippet":"arXiv:2602.11661v1 Announce Type: new \nAbstract: While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics","link":"https://arxiv.org/abs/2602.11666","snippet":"arXiv:2602.11666v1 Announce Type: new \nAbstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to \"context poisoning,\" where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs","link":"https://arxiv.org/abs/2602.11674","snippet":"arXiv:2602.11674v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.","publishedAt":"2026-02-13T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"}]}